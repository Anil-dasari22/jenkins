{
  "cells": [
    {
      "metadata": {
        "id": "wdj9RMfoGPC2"
      },
      "cell_type": "markdown",
      "source": [
        "Colab is making it easier than ever to integrate powerful Generative AI capabilities into your projects. We are launching public preview for a simple and intuitive Python library (google.colab.ai) to access state-of-the-art language models directly within Pro and Pro+ subscriber Colab environments.  This means subscribers can spend less time on configuration and set up and more time bringing their ideas to life. With just a few lines of code, you can now perform a variety of tasks:\n",
        "- Generate text\n",
        "- Translate languages\n",
        "- Write creative content\n",
        "- Categorize text\n",
        "\n",
        "Happy Coding!\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/Getting_started_with_google_colab_ai.ipynb)"
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHO9VzO9AHZP",
        "outputId": "5227be6c-5670-4b88-a899-0e14b828903c"
      },
      "cell_type": "code",
      "source": [
        " import json\n",
        "\n",
        "\n",
        "environment_data = {\n",
        "  \"A\": \"Dirty\",\n",
        "  \"B\": \"Clean\"\n",
        "}\n",
        "\n",
        "with open(\"vacuum_environment.json\", \"w\") as f:\n",
        "  json.dump(environment_data, f)\n",
        "\n",
        "print(\"Created vacuum_environment.json with sample data.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created vacuum_environment.json with sample data.\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "Ucchuu5vV3Jp"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "class SimpleReflexAgent:\n",
        "    def __init__(self, env):\n",
        "      self.location = 'A'\n",
        "      self.environment = env\n",
        "\n",
        "    def perceive(self):\n",
        "      return self.environment, self.environment[self.location]\n",
        "\n",
        "    def act(self, percept):\n",
        "      loc, status = percept\n",
        "      if status == 'Dirty':\n",
        "        self.environment[loc] = \"Clean\"\n",
        "        return \"Clean\"\n",
        "      else:\n",
        "        self.location = 'B' if loc == 'A' else 'A'\n",
        "        return f\"Move to {self.location}\"\n",
        "\n",
        "def main():\n",
        "  with open(\"vacuum_environment.json\") as f:\n",
        "    environment = json.load(f)\n",
        "\n",
        "  agent = SimpleReflexAgent(environment)\n",
        "\n",
        "  for _ in range(4):\n",
        "    percept = agent.perceive()\n",
        "    action = agent.act(percept)\n",
        "    print(f\"Room {agent.location} - {environment[agent.location]}\")\n",
        "    print(f\"Action: {action}\\n\")"
      ],
      "outputs": [],
      "execution_count": 18
    },
    {
      "metadata": {
        "id": "LjfCGEpzDsD9"
      },
      "cell_type": "markdown",
      "source": [
        "Choosing a Model\n",
        "The model names give you a hint about their capabilities and intended use:\n",
        "\n",
        "Pro: These are the most capable models, ideal for complex reasoning, creative tasks, and detailed analysis.\n",
        "\n",
        "Flash: These models are optimized for high speed and efficiency, making them great for summarization, chat applications, and tasks requiring rapid responses.\n",
        "\n",
        "Gemma: These are lightweight, open-weight models suitable for a variety of text generation tasks and are great for experimentation."
      ]
    },
    {
      "metadata": {
        "id": "R7taibpc7x2l"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "outputs": [],
      "execution_count": 18
    },
    {
      "metadata": {
        "id": "ysDdFbH_Dgtz"
      },
      "cell_type": "markdown",
      "source": [
        "For longer text generations, you can stream the response. This displays the output token by token as it's generated, rather than waiting for the entire response to complete. This provides a more interactive and responsive experience. To enable this, simply set stream=True."
      ]
    },
    {
      "metadata": {
        "id": "4BNgxiB6--_5"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CpMmpaVClSBV"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "DWiLPzTnRoy-"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RBMCpgL20J2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}